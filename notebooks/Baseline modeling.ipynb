{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\nimport statsmodels.api as sm\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom itertools import product\n\nfrom xgboost import XGBRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import SGDRegressor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-11T14:26:47.127926Z","iopub.execute_input":"2023-02-11T14:26:47.128437Z","iopub.status.idle":"2023-02-11T14:26:49.699366Z","shell.execute_reply.started":"2023-02-11T14:26:47.128334Z","shell.execute_reply":"2023-02-11T14:26:49.698343Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class ETL:\n    def __init__(self):\n        self.common_data_path = '../input/competitive-data-science-predict-future-sales'\n\n        self.sales_train = None\n        self.shops = None\n        self.test = None\n        self.item_categories = None\n        self.items = None\n\n    def extract(self) -> None:\n        self.sales_train = pd.read_csv(f'{self.common_data_path}/sales_train.csv')\n        self.shops = pd.read_csv(f'{self.common_data_path}/shops.csv')\n        self.test = pd.read_csv(f'{self.common_data_path}/test.csv')\n        self.item_categories = pd.read_csv(f'{self.common_data_path}/item_categories.csv')\n        self.items = pd.read_csv(f'{self.common_data_path}/items.csv')\n\n    def transform(self) -> None:\n        self.sales_train.date = pd.to_datetime(self.sales_train.date)\n        self.sales_train['month'] = self.sales_train.date.dt.to_period('M')\n\n        self.sales_train = self.sales_train[self.sales_train.item_cnt_day > -2]\n        self.sales_train = self.sales_train[self.sales_train.item_price < 300_000]\n        self.sales_train = self.sales_train[self.sales_train.month < '2015-11']\n\n        id_of_duplicated_shops = {10: 11, 0: 57, 1: 58, 40: 39}\n\n        for k, v in id_of_duplicated_shops.items():\n            self.shops = self.shops[self.shops.shop_id != k]\n            self.sales_train.loc[self.sales_train.shop_id == k, 'shop_id'] = v\n\n        self.test.loc[self.test.shop_id == 10, 'shop_id'] = 11\n\n        self.shops.shop_name = self.shops.shop_name.map(lambda x: x.lstrip('!'))\n\n        self.shops['city'] = self.shops.shop_name.str.split(' ').map(lambda x: x[0])\n        self.shops['category'] = self.shops.shop_name.str.split(' ').map(lambda x: x[1])\n\n        shop_enc = LabelEncoder()\n        self.shops['shop_category'] = shop_enc.fit_transform(self.shops['category'])\n        shop_city_enc = LabelEncoder()\n        self.shops['shop_city'] = shop_city_enc.fit_transform(self.shops['city'])\n        self.shops = self.shops[['shop_id', 'shop_category', 'shop_city']]\n\n        self.sales_train = self.sales_train.drop_duplicates(\n            subset=['date', 'date_block_num', 'shop_id', 'item_id', 'item_price', 'item_cnt_day'],\n            keep='last')\n\n        self.items.item_name = self.items.item_name.map(lambda x: x.lstrip('!*/'))\n\n        same_items_regardless_case = self.items[self.items.item_name.map(lambda x: x.lower()).duplicated()]\n        fully_same = self.items.groupby(self.items.item_name, as_index=False).size().query('size > 1')\n        fully_same_item_names = fully_same.item_name.values\n\n        self.items = self.items[self.items.item_id != 12]\n        self.sales_train = self.sales_train[self.sales_train.item_id != 12]\n\n        same_items_regardless_case_names = same_items_regardless_case[~same_items_regardless_case.item_name.isin(\n            fully_same_item_names)].item_name\n\n        same_items = self.items[self.items.item_name.map(lambda x: x.lower()).isin(\n            same_items_regardless_case_names.map(lambda x: x.lower()))]\n\n        same_items_upper_case = same_items[same_items.item_name.str.endswith('(Регион)')]\n        same_items_lower_case = same_items[same_items.item_name.str.endswith('(регион)')]\n\n        self.items = self.items[~self.items.item_id.isin(same_items_lower_case.item_id)]\n\n        self.sales_train.loc[self.sales_train.item_id.isin(same_items_lower_case.item_id), 'item_id'] = \\\n            self.sales_train[self.sales_train.item_id.isin(same_items_lower_case.item_id)].item_id.map(lambda x: x - 1)\n\n        self.test.loc[self.test.item_id.isin(same_items_lower_case.item_id), 'item_id'] = \\\n            self.test[self.test.item_id.isin(same_items_lower_case.item_id)].item_id.map(lambda x: x - 1)\n\n        self.items = self.items[self.items.item_id != 13012]\n        self.sales_train.loc[self.sales_train.item_id == 13012, 'item_id'] = 13011\n        self.test.loc[self.test.item_id == 13012, 'item_id'] = 13011\n\n        self.items['name1'], self.items['name2'] = self.items['item_name'].str.split('[', 1).str\n        self.items['name1'], self.items['name3'] = self.items['item_name'].str.split('(', 1).str\n\n        self.items['name2'] = self.items['name2'].str.replace('\\W+', ' ').str.lower()\n        self.items['name3'] = self.items['name3'].str.replace('\\W+', ' ').str.lower()\n\n        self.items = self.items.fillna('0')\n\n        self.items['name2'] = LabelEncoder().fit_transform(self.items['name2'])\n        self.items['name3'] = LabelEncoder().fit_transform(self.items['name3'])\n\n        self.items.drop(['item_name', 'name1'], axis=1, inplace=True)\n\n        self.item_categories['type_code'] = self.item_categories['item_category_name'].apply(\n            lambda x: x.split(' ')[0]).astype(str)\n\n        categories = []\n        for cat in self.item_categories['type_code'].unique():\n            if len(self.item_categories[self.item_categories['type_code'] == cat]) > 3:\n                categories.append(cat)\n        self.item_categories['type_code'] = self.item_categories['type_code'].apply(\n            lambda c: c if c in categories else 'other')\n\n        self.item_categories['type_code'] = LabelEncoder().fit_transform(self.item_categories['type_code'])\n        self.item_categories['subcat'] = self.item_categories['item_category_name'].apply(lambda x: x.split('-')).apply(\n            lambda x: x[1].strip() if len(x) >= 2 else x[0].strip())\n\n        self.item_categories['subcat'] = LabelEncoder().fit_transform(self.item_categories['subcat'])\n        self.item_categories.drop('item_category_name', axis=1, inplace=True)\n\n    def load(self):\n        \n        return self.sales_train, self.shops, self.test, self.item_categories, self.items","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:26:49.702275Z","iopub.execute_input":"2023-02-11T14:26:49.702788Z","iopub.status.idle":"2023-02-11T14:26:49.742571Z","shell.execute_reply.started":"2023-02-11T14:26:49.702736Z","shell.execute_reply":"2023-02-11T14:26:49.741280Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"etl = ETL()\netl.extract()\netl.transform()\n\nsales, shops, test, item_categories, items = etl.load()","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:27:01.576650Z","iopub.execute_input":"2023-02-11T14:27:01.578100Z","iopub.status.idle":"2023-02-11T14:27:05.895120Z","shell.execute_reply.started":"2023-02-11T14:27:01.578034Z","shell.execute_reply":"2023-02-11T14:27:05.893850Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:79: FutureWarning: Columnar iteration over characters will be deprecated in future releases.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:80: FutureWarning: Columnar iteration over characters will be deprecated in future releases.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:82: FutureWarning: The default value of regex will change from True to False in a future version.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:83: FutureWarning: The default value of regex will change from True to False in a future version.\n","output_type":"stream"}]},{"cell_type":"code","source":"class RollingWindowCV(object):\n    \n    def __init__(self, train_period=12, test_period=1, gap=1):\n        self.n_splits = 0\n        self.train_period = train_period\n        self.test_period = test_period\n        self.gap = gap\n    \n    def split(self, data, \n              date_column='date_block_num', \n              target_column='item_cnt_month', \n              clip_range: list = [0, 20]):\n\n        try:\n            data[date_column]\n        except:\n            raise KeyError(date_column)\n        \n        start_train = int(data[date_column].min())\n        end_train = start_train + self.train_period\n        start_test = end_train + self.gap\n        end_test = start_test + self.test_period\n\n        while end_test < data[date_column].max():\n            train_indices = list(data[(data[date_column] >= start_train) & \n                                     (data[date_column] < end_train)].index)\n\n\n            test_indices = list(data[(data[date_column] >= start_test) &\n                                    (data[date_column] < end_test)].index)\n            \n            print(\"Train period:\", start_train,\"-\" , end_train, \", Test period\", start_test, \"-\", end_test,\n                  \"# train records\", len(train_indices), \", # test records\", len(test_indices))\n\n            start_train = end_train + 1\n            end_train = start_train + self.train_period\n            start_test = end_train + self.gap\n            end_test = start_test + self.test_period\n            \n            self.n_splits += 1\n            \n            train_data = data.loc[train_indices]\n            X_train = train_data.drop([target_column], axis=1).fillna(0)\n            y_train = train_data[target_column].clip(*clip_range)\n            \n            valid_data = data.loc[test_indices]\n            X_valid = valid_data.drop([target_column], axis=1).fillna(0)\n            y_valid = valid_data[target_column].clip(*clip_range)\n            \n            yield X_train, y_train, X_valid, y_valid\n    \n    \n    def get_n_splits(self):\n        return self.n_splits ","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:33:37.708323Z","iopub.execute_input":"2023-02-11T14:33:37.708776Z","iopub.status.idle":"2023-02-11T14:33:37.723877Z","shell.execute_reply.started":"2023-02-11T14:33:37.708741Z","shell.execute_reply":"2023-02-11T14:33:37.722439Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class TrainEngineering:\n    def __init__(self, data):\n        self.data: pd.DataFrame = data\n        self.train: pd.DataFrame = None\n        \n    def create_train(self,\n                     cols: list, \n                     data_column: str, \n                     unique_cols: list):\n        train = []\n        for data_block in range(self.data[data_column].nunique()):\n            train_temp = self.data[self.data[data_column] == data_block]\n            train.append(np.array(list(product([data_block], *[train_temp[unique_col].unique() \n                                                                  for unique_col in unique_cols]))))\n\n        train = pd.DataFrame(np.vstack(train), columns=cols)\n        self.train = train.sort_values(cols)\n        \n    def add_aggregated_data(self,\n                            groupby_cols: list,\n                            agg_col_funcs: dict,\n                            agg_cols_name: list):\n        aggregated_data = self.data.groupby(groupby_cols).agg(agg_col_funcs)\n        aggregated_data.columns = agg_cols_name\n        aggregated_data.reset_index(inplace=True)\n        \n        self.merge_tables([(groupby_cols, aggregated_data)])\n        self.train = self.train.fillna(0)\n        \n    def add_lags(self, periods, lag_cols, \n                 date_block_column='date_block_num', \n                 agg_cols=['date_block_num', 'shop_id', 'item_id']):\n        for lag_col in lag_cols:\n            aggregated = self.train[[*agg_cols, lag_col]]\n            for per in periods:\n                shifted = aggregated.copy()\n                shifted.columns = [*agg_cols, lag_col + \"_lag_\" + str(per)]\n                shifted[date_block_column] = shifted[date_block_column] + per\n                self.merge_tables([(agg_cols, shifted)])\n                self.train = self.train.fillna(0)\n        \n    def merge_tables(self, table_keys: list):\n        for key, table in table_keys:\n            self.train = pd.merge(self.train, table, on=key, how='left')\n    \n    def concat(self, data, cols: list):\n        self.train = pd.concat([self.train, data], \n                               ignore_index=True, \n                               sort=False, \n                               keys=cols).fillna(0)\n            \n    def get_train(self):\n        return self.train","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:33:38.435179Z","iopub.execute_input":"2023-02-11T14:33:38.436054Z","iopub.status.idle":"2023-02-11T14:33:38.450747Z","shell.execute_reply.started":"2023-02-11T14:33:38.436007Z","shell.execute_reply":"2023-02-11T14:33:38.449437Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class TestEngineering:\n    def __init__(self, data):\n        self.test: pd.DataFrame = data\n            \n    def select_features(self, cols: list):\n        self.test = self.test[cols]\n        \n    def add_column(self, col: str, values: any):\n        self.test[col] = values\n\n    def get_test(self):\n        return self.test","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:33:38.696648Z","iopub.execute_input":"2023-02-11T14:33:38.697141Z","iopub.status.idle":"2023-02-11T14:33:38.704717Z","shell.execute_reply.started":"2023-02-11T14:33:38.697101Z","shell.execute_reply":"2023-02-11T14:33:38.703426Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class ModelCV:\n    def __init__(self,\n                 train_data: pd.DataFrame,\n                 test_data: pd.DataFrame,\n                 items: pd.DataFrame, \n                 item_categories: pd.DataFrame, \n                 shops: pd.DataFrame,\n                 train_eng_class, \n                 test_eng_class, \n                 cv_class):\n        self.train_data = train_data\n        self.test_data = test_data\n        self.items = items\n        self.item_categories = item_categories\n        self.shops = shops\n        self.train_eng_class = train_eng_class\n        self.test_eng_class = test_eng_class\n        self.cv_class = cv_class\n        \n        self.train_eng = None\n        self.test_eng = None\n        \n    def create_train_table(self):\n        self.train_eng = self.train_eng_class(self.train_data)\n\n        self.train_eng.create_train(['date_block_num', 'shop_id', 'item_id'], \n                               'date_block_num', \n                               ['shop_id', 'item_id'])\n\n        self.train_eng.add_aggregated_data(['date_block_num', 'shop_id', 'item_id'], \n                                      {'item_cnt_day': 'sum'},\n                                      ['item_cnt_month'])\n        \n        self.test_eng = self.test_eng_class(self.test_data)\n        \n        self.test_eng.select_features(['shop_id', 'item_id'])\n        self.test_eng.add_column('date_block_num', self.train_data.date_block_num.max() + 1)\n        \n        self.train_eng.concat(self.test_eng.get_test(), ['date_block_num', 'shop_id', 'item_id'])\n        self.train = self.train_eng.get_train()\n        \n        self.train_eng.merge_tables([('item_id', self.items), \n                        ('item_category_id', self.item_categories),\n                        ('shop_id', self.shops)])\n        \n        self.train_eng.add_lags([1, 2, 12], ['item_cnt_month'])\n        \n        self.train_data = self.train_eng.get_train()\n        \n    def get_splits(self, traget_col: str):\n        cv = self.cv_class()\n        for X_train, y_train, X_valid, y_valid in cv.split(self.train_data):\n            if X_train is not None:\n                yield X_train, y_train, X_valid, y_valid\n            else: \n                raise StopIteration","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:45:42.922412Z","iopub.execute_input":"2023-02-11T14:45:42.922913Z","iopub.status.idle":"2023-02-11T14:45:42.937338Z","shell.execute_reply.started":"2023-02-11T14:45:42.922871Z","shell.execute_reply":"2023-02-11T14:45:42.936439Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"tscv = ModelCV(sales, test, items, \n               item_categories, shops, TrainEngineering, \n               TestEngineering, RollingWindowCV)\ntscv.create_train_table()\nmodels = []\nfor X_train, y_train, X_valid, y_valid in tscv.get_splits('item_cnt_month'):\n\n    model = XGBRegressor(\n       max_depth=8,\n       n_estimators=1000,\n       min_child_weight=0.8, \n       colsample_bytree=0.8, \n       subsample=0.8, \n       eta=0.1,\n       used_ram_limit= \"13gb\",\n    )\n\n    model.fit(\n       X_train, \n       y_train, \n       eval_metric='rmse',\n       eval_set=[(X_train, y_train), (X_valid, y_valid)], \n       verbose=True, \n       early_stopping_rounds=10\n    )\n    \n    error = np.sqrt(mean_squared_error(y_valid, model.predict(X_valid), squared=False))\n    \n    models.append((error, model))","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:45:44.403942Z","iopub.execute_input":"2023-02-11T14:45:44.404696Z","iopub.status.idle":"2023-02-11T14:50:01.401108Z","shell.execute_reply.started":"2023-02-11T14:45:44.404641Z","shell.execute_reply":"2023-02-11T14:50:01.399232Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Train period: 0 - 12 , Test period 13 - 14 # train records 4486019 , # test records 326922\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:797: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  UserWarning,\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:797: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  UserWarning,\n","output_type":"stream"},{"name":"stdout","text":"[14:46:52] WARNING: ../src/learner.cc:627: \nParameters: { \"used_ram_limit\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\tvalidation_0-rmse:1.24281\tvalidation_1-rmse:1.20520\n[1]\tvalidation_0-rmse:1.20639\tvalidation_1-rmse:1.14942\n[2]\tvalidation_0-rmse:1.17516\tvalidation_1-rmse:1.10182\n[3]\tvalidation_0-rmse:1.15738\tvalidation_1-rmse:1.06735\n[4]\tvalidation_0-rmse:1.13390\tvalidation_1-rmse:1.03659\n[5]\tvalidation_0-rmse:1.12106\tvalidation_1-rmse:1.01505\n[6]\tvalidation_0-rmse:1.11025\tvalidation_1-rmse:0.99971\n[7]\tvalidation_0-rmse:1.10107\tvalidation_1-rmse:0.98829\n[8]\tvalidation_0-rmse:1.08599\tvalidation_1-rmse:0.97679\n[9]\tvalidation_0-rmse:1.07957\tvalidation_1-rmse:0.97308\n[10]\tvalidation_0-rmse:1.06838\tvalidation_1-rmse:0.96575\n[11]\tvalidation_0-rmse:1.05805\tvalidation_1-rmse:0.96221\n[12]\tvalidation_0-rmse:1.04970\tvalidation_1-rmse:0.96029\n[13]\tvalidation_0-rmse:1.04295\tvalidation_1-rmse:0.95791\n[14]\tvalidation_0-rmse:1.03700\tvalidation_1-rmse:0.95870\n[15]\tvalidation_0-rmse:1.03182\tvalidation_1-rmse:0.95767\n[16]\tvalidation_0-rmse:1.02741\tvalidation_1-rmse:0.96052\n[17]\tvalidation_0-rmse:1.02344\tvalidation_1-rmse:0.96323\n[18]\tvalidation_0-rmse:1.01998\tvalidation_1-rmse:0.96609\n[19]\tvalidation_0-rmse:1.01717\tvalidation_1-rmse:0.96683\n[20]\tvalidation_0-rmse:1.01483\tvalidation_1-rmse:0.97150\n[21]\tvalidation_0-rmse:1.01287\tvalidation_1-rmse:0.97238\n[22]\tvalidation_0-rmse:1.01104\tvalidation_1-rmse:0.97294\n[23]\tvalidation_0-rmse:1.00927\tvalidation_1-rmse:0.97417\n[24]\tvalidation_0-rmse:1.00768\tvalidation_1-rmse:0.97771\n[25]\tvalidation_0-rmse:1.00632\tvalidation_1-rmse:0.98020\nTrain period: 13 - 25 , Test period 26 - 27 # train records 3810545 , # test records 274712\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:797: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n  UserWarning,\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:797: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  UserWarning,\n","output_type":"stream"},{"name":"stdout","text":"[14:47:27] WARNING: ../src/learner.cc:627: \nParameters: { \"used_ram_limit\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\tvalidation_0-rmse:1.21356\tvalidation_1-rmse:1.03962\n[1]\tvalidation_0-rmse:1.18009\tvalidation_1-rmse:1.00231\n[2]\tvalidation_0-rmse:1.14284\tvalidation_1-rmse:0.96772\n[3]\tvalidation_0-rmse:1.11842\tvalidation_1-rmse:0.94205\n[4]\tvalidation_0-rmse:1.09770\tvalidation_1-rmse:0.92216\n[5]\tvalidation_0-rmse:1.08117\tvalidation_1-rmse:0.90550\n[6]\tvalidation_0-rmse:1.05776\tvalidation_1-rmse:0.88839\n[7]\tvalidation_0-rmse:1.04928\tvalidation_1-rmse:0.88132\n[8]\tvalidation_0-rmse:1.03856\tvalidation_1-rmse:0.87233\n[9]\tvalidation_0-rmse:1.02108\tvalidation_1-rmse:0.86124\n[10]\tvalidation_0-rmse:1.00616\tvalidation_1-rmse:0.85249\n[11]\tvalidation_0-rmse:0.99397\tvalidation_1-rmse:0.84495\n[12]\tvalidation_0-rmse:0.98377\tvalidation_1-rmse:0.83714\n[13]\tvalidation_0-rmse:0.97519\tvalidation_1-rmse:0.83226\n[14]\tvalidation_0-rmse:0.96809\tvalidation_1-rmse:0.82794\n[15]\tvalidation_0-rmse:0.96205\tvalidation_1-rmse:0.82380\n[16]\tvalidation_0-rmse:0.95691\tvalidation_1-rmse:0.82095\n[17]\tvalidation_0-rmse:0.95330\tvalidation_1-rmse:0.81873\n[18]\tvalidation_0-rmse:0.94914\tvalidation_1-rmse:0.81658\n[19]\tvalidation_0-rmse:0.94550\tvalidation_1-rmse:0.81479\n[20]\tvalidation_0-rmse:0.94227\tvalidation_1-rmse:0.81383\n[21]\tvalidation_0-rmse:0.93908\tvalidation_1-rmse:0.81259\n[22]\tvalidation_0-rmse:0.93653\tvalidation_1-rmse:0.81215\n[23]\tvalidation_0-rmse:0.93472\tvalidation_1-rmse:0.81149\n[24]\tvalidation_0-rmse:0.93271\tvalidation_1-rmse:0.81072\n[25]\tvalidation_0-rmse:0.93078\tvalidation_1-rmse:0.81039\n[26]\tvalidation_0-rmse:0.92901\tvalidation_1-rmse:0.80990\n[27]\tvalidation_0-rmse:0.92734\tvalidation_1-rmse:0.80976\n[28]\tvalidation_0-rmse:0.92505\tvalidation_1-rmse:0.80920\n[29]\tvalidation_0-rmse:0.92368\tvalidation_1-rmse:0.80866\n[30]\tvalidation_0-rmse:0.92209\tvalidation_1-rmse:0.80847\n[31]\tvalidation_0-rmse:0.92095\tvalidation_1-rmse:0.80799\n[32]\tvalidation_0-rmse:0.91875\tvalidation_1-rmse:0.80783\n[33]\tvalidation_0-rmse:0.91755\tvalidation_1-rmse:0.80718\n[34]\tvalidation_0-rmse:0.91634\tvalidation_1-rmse:0.80693\n[35]\tvalidation_0-rmse:0.91520\tvalidation_1-rmse:0.80678\n[36]\tvalidation_0-rmse:0.91246\tvalidation_1-rmse:0.80734\n[37]\tvalidation_0-rmse:0.91143\tvalidation_1-rmse:0.80708\n[38]\tvalidation_0-rmse:0.90902\tvalidation_1-rmse:0.80849\n[39]\tvalidation_0-rmse:0.90801\tvalidation_1-rmse:0.80826\n[40]\tvalidation_0-rmse:0.90705\tvalidation_1-rmse:0.80825\n[41]\tvalidation_0-rmse:0.90613\tvalidation_1-rmse:0.80799\n[42]\tvalidation_0-rmse:0.90385\tvalidation_1-rmse:0.80830\n[43]\tvalidation_0-rmse:0.90306\tvalidation_1-rmse:0.80790\n[44]\tvalidation_0-rmse:0.90173\tvalidation_1-rmse:0.80797\n","output_type":"stream"}]},{"cell_type":"code","source":"final_model =  min(models, key = lambda score: score[0])[1]","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:54:45.652891Z","iopub.execute_input":"2023-02-11T14:54:45.654144Z","iopub.status.idle":"2023-02-11T14:54:45.660182Z","shell.execute_reply.started":"2023-02-11T14:54:45.654088Z","shell.execute_reply":"2023-02-11T14:54:45.658677Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train = tscv.train_eng.get_train()","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:55:11.757498Z","iopub.execute_input":"2023-02-11T14:55:11.757991Z","iopub.status.idle":"2023-02-11T14:55:11.763555Z","shell.execute_reply.started":"2023-02-11T14:55:11.757952Z","shell.execute_reply":"2023-02-11T14:55:11.762633Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"X_test = train[train.date_block_num == 34]","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:55:13.747072Z","iopub.execute_input":"2023-02-11T14:55:13.747632Z","iopub.status.idle":"2023-02-11T14:55:13.784939Z","shell.execute_reply.started":"2023-02-11T14:55:13.747573Z","shell.execute_reply":"2023-02-11T14:55:13.783539Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"X_test = X_test.drop('item_cnt_month', axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:55:15.648404Z","iopub.execute_input":"2023-02-11T14:55:15.649523Z","iopub.status.idle":"2023-02-11T14:55:15.661671Z","shell.execute_reply.started":"2023-02-11T14:55:15.649480Z","shell.execute_reply":"2023-02-11T14:55:15.660301Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"X_test = X_test.fillna(0)","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:55:41.088941Z","iopub.execute_input":"2023-02-11T14:55:41.089390Z","iopub.status.idle":"2023-02-11T14:55:41.101601Z","shell.execute_reply.started":"2023-02-11T14:55:41.089357Z","shell.execute_reply":"2023-02-11T14:55:41.100501Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"predictions = final_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:55:43.146935Z","iopub.execute_input":"2023-02-11T14:55:43.147389Z","iopub.status.idle":"2023-02-11T14:55:43.285960Z","shell.execute_reply.started":"2023-02-11T14:55:43.147347Z","shell.execute_reply":"2023-02-11T14:55:43.284826Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"submission = {'ID': test.index, 'item_cnt_month': predictions}\nsubmission = pd.DataFrame(submission)","metadata":{"execution":{"iopub.status.busy":"2023-02-11T15:00:20.814251Z","iopub.execute_input":"2023-02-11T15:00:20.814920Z","iopub.status.idle":"2023-02-11T15:00:20.824255Z","shell.execute_reply.started":"2023-02-11T15:00:20.814875Z","shell.execute_reply":"2023-02-11T15:00:20.822625Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-11T15:00:23.486163Z","iopub.execute_input":"2023-02-11T15:00:23.486832Z","iopub.status.idle":"2023-02-11T15:00:23.921548Z","shell.execute_reply.started":"2023-02-11T15:00:23.486761Z","shell.execute_reply":"2023-02-11T15:00:23.920385Z"},"trusted":true},"execution_count":30,"outputs":[]}]}